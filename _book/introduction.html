<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 Introduction | A Minimal Book Example</title>
  <meta name="description" content="This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook." />
  <meta name="generator" content="bookdown 0.14 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 Introduction | A Minimal Book Example" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 Introduction | A Minimal Book Example" />
  
  <meta name="twitter:description" content="This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook." />
  

<meta name="author" content="Steve Pittard" />


<meta name="date" content="2020-02-17" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="index.html"/>

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Minimal Book Example</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>2</b> Introduction</a><ul>
<li class="chapter" data-level="2.1" data-path="introduction.html"><a href="introduction.html#performance-metrics"><i class="fa fa-check"></i><b>2.1</b> Performance Metrics</a></li>
<li class="chapter" data-level="2.2" data-path="introduction.html"><a href="introduction.html#in-sample-vs-out-of-sample-data"><i class="fa fa-check"></i><b>2.2</b> In-Sample vs Out-Of-Sample Data</a></li>
<li class="chapter" data-level="2.3" data-path="introduction.html"><a href="introduction.html#a-practical-example"><i class="fa fa-check"></i><b>2.3</b> A Practical Example</a><ul>
<li class="chapter" data-level="2.3.1" data-path="introduction.html"><a href="introduction.html#important-terminology"><i class="fa fa-check"></i><b>2.3.1</b> Important Terminology</a></li>
<li class="chapter" data-level="2.3.2" data-path="introduction.html"><a href="introduction.html#exploratory-plots"><i class="fa fa-check"></i><b>2.3.2</b> Exploratory Plots</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="introduction.html"><a href="introduction.html#a-common-modeling-workflow"><i class="fa fa-check"></i><b>2.4</b> A Common Modeling Workflow</a></li>
<li class="chapter" data-level="2.5" data-path="introduction.html"><a href="introduction.html#splitting-the-data"><i class="fa fa-check"></i><b>2.5</b> Splitting The Data</a></li>
<li class="chapter" data-level="2.6" data-path="introduction.html"><a href="introduction.html#first-model"><i class="fa fa-check"></i><b>2.6</b> First Model</a><ul>
<li class="chapter" data-level="2.6.1" data-path="introduction.html"><a href="introduction.html#first-prediction"><i class="fa fa-check"></i><b>2.6.1</b> First Prediction</a></li>
<li class="chapter" data-level="2.6.2" data-path="introduction.html"><a href="introduction.html#confusion-matrices"><i class="fa fa-check"></i><b>2.6.2</b> Confusion Matrices</a></li>
<li class="chapter" data-level="2.6.3" data-path="introduction.html"><a href="introduction.html#performance-measures"><i class="fa fa-check"></i><b>2.6.3</b> Performance Measures</a></li>
<li class="chapter" data-level="2.6.4" data-path="introduction.html"><a href="introduction.html#the-roc-curve"><i class="fa fa-check"></i><b>2.6.4</b> The ROC curve</a></li>
<li class="chapter" data-level="2.6.5" data-path="introduction.html"><a href="introduction.html#other-methods"><i class="fa fa-check"></i><b>2.6.5</b> Other Methods ?</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="introduction.html"><a href="introduction.html#improving-the-models"><i class="fa fa-check"></i><b>2.7</b> Improving The Model(s)</a><ul>
<li class="chapter" data-level="2.7.1" data-path="introduction.html"><a href="introduction.html#cross-fold-validation"><i class="fa fa-check"></i><b>2.7.1</b> Cross Fold Validation</a></li>
<li class="chapter" data-level="2.7.2" data-path="introduction.html"><a href="introduction.html#is-there-a-better-way"><i class="fa fa-check"></i><b>2.7.2</b> Is There a Better Way ?</a></li>
<li class="chapter" data-level="2.7.3" data-path="introduction.html"><a href="introduction.html#data-splitting-using-caret"><i class="fa fa-check"></i><b>2.7.3</b> Data Splitting Using Caret</a></li>
<li class="chapter" data-level="2.7.4" data-path="introduction.html"><a href="introduction.html#specifying-control-options"><i class="fa fa-check"></i><b>2.7.4</b> Specifying Control Options</a></li>
<li class="chapter" data-level="2.7.5" data-path="introduction.html"><a href="introduction.html#inspecting-the-model"><i class="fa fa-check"></i><b>2.7.5</b> Inspecting The Model</a></li>
<li class="chapter" data-level="2.7.6" data-path="introduction.html"><a href="introduction.html#how-well-did-it-perform"><i class="fa fa-check"></i><b>2.7.6</b> How Well Did It Perform ?</a></li>
<li class="chapter" data-level="2.7.7" data-path="introduction.html"><a href="introduction.html#comparing-performance-across-other-methods"><i class="fa fa-check"></i><b>2.7.7</b> Comparing Performance Across Other Methods</a></li>
<li class="chapter" data-level="2.7.8" data-path="introduction.html"><a href="introduction.html#different-performance-measures"><i class="fa fa-check"></i><b>2.7.8</b> Different Performance Measures</a></li>
<li class="chapter" data-level="2.7.9" data-path="introduction.html"><a href="introduction.html#feature-importance"><i class="fa fa-check"></i><b>2.7.9</b> Feature Importance</a></li>
<li class="chapter" data-level="2.7.10" data-path="introduction.html"><a href="introduction.html#feature-elimination"><i class="fa fa-check"></i><b>2.7.10</b> Feature Elimination</a></li>
<li class="chapter" data-level="2.7.11" data-path="introduction.html"><a href="introduction.html#the-rfe-function"><i class="fa fa-check"></i><b>2.7.11</b> The rfe Function</a></li>
<li class="chapter" data-level="2.7.12" data-path="introduction.html"><a href="introduction.html#comparing-models"><i class="fa fa-check"></i><b>2.7.12</b> Comparing Models</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">A Minimal Book Example</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="introduction" class="section level1">
<h1><span class="header-section-number">Chapter 2</span> Introduction</h1>
<p>This lecture is concerned primarily with Predictive Modeling. In this domain there are generally two types of predictive models:</p>
<ol style="list-style-type: decimal">
<li>Classification for Predicting Qualitative Outcomes:</li>
</ol>
<p>This relates to situations such as whether someone has a disease (“positive”) or not (“negative”). The problem could also be multi classification such as assigning an organism to one of a number of possible species.</p>
<ol start="2" style="list-style-type: decimal">
<li>Regression for Quantitative Out Comes</li>
</ol>
<p>This is when we wish to predict a numeric / continuous outcome such as a final sales price for a house or car. It might also be a prediction of tomorrow’s stock or Bit Coin price.</p>
<div id="performance-metrics" class="section level2">
<h2><span class="header-section-number">2.1</span> Performance Metrics</h2>
<p>For either of those cases we need some type of metric or measure to let us know how well a given model will work on new or unseen data - also known as “out of sample” data. for Classification problems we look at things like “sensitivity”, “specificity”, “accuracy”, and “Area Under Curve”. For Quantitative outcomes, we look at things like Root Mean Square Error (RMSE) or Mean Absolute Error (MAE). The selection of metric will frequently depend on your domain of interest.</p>
</div>
<div id="in-sample-vs-out-of-sample-data" class="section level2">
<h2><span class="header-section-number">2.2</span> In-Sample vs Out-Of-Sample Data</h2>
<p>Independently of the metric, the goal of predictive model is to generate models that can generalize to new data. It would be good if any model we generate could provide a good estimate of out of sample error. It’s easy to generate a model on an entire data set (in sample data) and then turn around and use that data for prediction. But how will it perform on new data ? Haven’t we just over trained our model ?</p>
</div>
<div id="a-practical-example" class="section level2">
<h2><span class="header-section-number">2.3</span> A Practical Example</h2>
<p>Let’s consider the Pima Indians Data that is part of the <strong>mlbench</strong> package. You can install this package via the Tools -&gt; Install Package menu item within RStudio or type the following at the R console prompt:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">install.packages</span>(<span class="st">&quot;mlbench&quot;</span>)</code></pre></div>
<p>Once you have it installed then load it into the work space as follows:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data</span>(<span class="st">&quot;PimaIndiansDiabetes&quot;</span>)

<span class="co"># Get a shorter handle. I hate typing. </span>
pm &lt;-<span class="st"> </span>PimaIndiansDiabetes</code></pre></div>
<p>The description of the data set is as follows:</p>
<div class="figure">
<img src="PICS/pima_desc.png" width="475" />

</div>
<p>So we now have some data on which we can build a model. Specifically, there is a variable in the data called “diabetes” which indicates the disease / diabetes status (“pos” or “neg”) of the person. It would be good to come up with a model that we could use with incoming data to determine if someone has diabetes.</p>
<div id="important-terminology" class="section level3">
<h3><span class="header-section-number">2.3.1</span> Important Terminology</h3>
<p>In predictive modeling there are some common terms to consider:</p>
<div class="figure">
<img src="PICS/features2.png" width="475" />

</div>
</div>
<div id="exploratory-plots" class="section level3">
<h3><span class="header-section-number">2.3.2</span> Exploratory Plots</h3>
<p>We’ll look use some stock plots from the <a href="https://github.com/elastacloud/automatic-data-explorer"><strong>DataExplorer</strong></a> package to get a feel for the data. Look at correlations between the variables to see if any are strongly correlated with the variable we wish to predict or any other variables.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot_correlation</span>(pm, <span class="dt">type=</span><span class="st">&quot;continuous&quot;</span>)</code></pre></div>
<p><img src="SEMINAR_SERIES_files/figure-html/decorr-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot_bar</span>(pm)</code></pre></div>
<p><img src="SEMINAR_SERIES_files/figure-html/debar-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot_histogram</span>(pm)</code></pre></div>
<p><img src="SEMINAR_SERIES_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot_boxplot</span>(pm,<span class="dt">by=</span><span class="st">&quot;diabetes&quot;</span>)</code></pre></div>
<p><img src="SEMINAR_SERIES_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
</div>
</div>
<div id="a-common-modeling-workflow" class="section level2">
<h2><span class="header-section-number">2.4</span> A Common Modeling Workflow</h2>
<p>The following graphic depicts the steps common to the Modeling Process. This is not the only way to proceed but it provides a very helpful schematic by which to plan your work.</p>
<div class="figure">
<img src="PICS/sworkflow.jpg" height="600" />

</div>
</div>
<div id="splitting-the-data" class="section level2">
<h2><span class="header-section-number">2.5</span> Splitting The Data</h2>
<p>A fundamental approach used in ML is to segment data into a “training” set which is some percentage of the original data - say 80%. The remaining 20% would be assigned to a “test” data set. Then we build a model on our training data set after which we use that model to predict outcomes for the test data set. This looks like the following.</p>
<div class="figure">
<img src="PICS/crossvalid.png" width="500" />

</div>
<p>Note that some scenarios will split the data into three data sets: 1) training, 2) validation, and 3) test. This scenario is used when tuning so called hyper parameters for methods that have “tuning” parameters that could influence the resulting model. We’ll stick with the basic “train / test” approach for now.</p>
<p>Splitting the data is not particularly challenging. We can use the built in <strong>sample</strong> function in R to do this. We aren’t sampling with replacement here which guarantees that no record can exist in both sets. That is, if a record from the data set is assigned to the training set, it will not be in the test data set.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Make this example reproducible</span>
<span class="kw">set.seed</span>(<span class="dv">123</span>) 
percent &lt;-<span class="st"> </span>.<span class="dv">80</span>

<span class="co"># Get the indices for a training set.</span>
idx &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(pm),<span class="kw">round</span>(.<span class="dv">8</span><span class="op">*</span><span class="kw">nrow</span>(pm)),F)

<span class="co"># Use bracket notation to create the train / test pair</span>
train &lt;-<span class="st"> </span>pm[idx,]
test  &lt;-<span class="st"> </span>pm[<span class="op">-</span>idx,]

<span class="co"># The following should have 80 percent of the original </span>
<span class="co"># data</span>

<span class="kw">round</span>(<span class="kw">nrow</span>(train)<span class="op">/</span><span class="kw">nrow</span>(pm)<span class="op">*</span><span class="dv">100</span>)</code></pre></div>
<pre><code>## [1] 80</code></pre>
</div>
<div id="first-model" class="section level2">
<h2><span class="header-section-number">2.6</span> First Model</h2>
<p>Now let’s build a Generalized Linear Model to do the prediction. We will employ logistic regression.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">myglm &lt;-<span class="st"> </span><span class="kw">glm</span>(diabetes <span class="op">~</span><span class="st"> </span>.,
             <span class="dt">data =</span> train,
             <span class="dt">family =</span> <span class="st">&quot;binomial&quot;</span>)

<span class="kw">summary</span>(myglm)</code></pre></div>
<pre><code>## 
## Call:
## glm(formula = diabetes ~ ., family = &quot;binomial&quot;, data = train)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.3941  -0.7235  -0.4285   0.7476   3.0031  
## 
## Coefficients:
##               Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) -8.2308564  0.7816436 -10.530  &lt; 2e-16 ***
## pregnant     0.1138202  0.0366475   3.106  0.00190 ** 
## glucose      0.0366854  0.0041947   8.746  &lt; 2e-16 ***
## pressure    -0.0131360  0.0059415  -2.211  0.02704 *  
## triceps     -0.0006303  0.0075466  -0.084  0.93343    
## insulin     -0.0017394  0.0009826  -1.770  0.07667 .  
## mass         0.0847273  0.0161080   5.260 1.44e-07 ***
## pedigree     0.9057850  0.3329203   2.721  0.00651 ** 
## age          0.0120925  0.0107367   1.126  0.26005    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 790.13  on 613  degrees of freedom
## Residual deviance: 581.40  on 605  degrees of freedom
## AIC: 599.4
## 
## Number of Fisher Scoring iterations: 5</code></pre>
<p>In looking at the output we see some problems such as a number of predictors aren’t significant so maybe we should eliminate them from the model. For now, we’ll keep going because we are trying to outline the larger process / workflow.</p>
<div id="first-prediction" class="section level3">
<h3><span class="header-section-number">2.6.1</span> First Prediction</h3>
<p>We could now use this new model to predict outcomes using the test data set. Remember that we are attempting to predict a binary outcome - in this case whether the person is positive for diabetes or negative.</p>
<p>What we get back from the prediction object are probabilities for which we have to determine a threshold above which we would say the observation is “positive” for diabetes and, below the threshold, “negative”.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">probs &lt;-<span class="st"> </span><span class="kw">predict</span>(myglm,
                 <span class="dt">newdata =</span> test,
                 <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>)

probs[<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>]</code></pre></div>
<pre><code>##         2         3         9        12        13        17        18 
## 0.0503311 0.8208652 0.6680994 0.9016430 0.7766679 0.3361188 0.2029466 
##        23        25        31 
## 0.9453408 0.6693923 0.4026717</code></pre>
<p>With logistic regression we are dealing with a curve like the one below which is a sigmoid function. The idea is to take our probabilities, which range between 0 and 1, and then pick a threshold over which we would classify that person as being positive for diabetes.</p>
<p><img src="SEMINAR_SERIES_files/figure-html/logitplot-1.png" width="672" /></p>
<p>The temptation is to select 0.5 as the threshold such that if a returned probability exceeds 0.5 then we classify the associated subject as being “positive” for the disease. But then this assumes that the probabilities are distributed accordingly. This is frequently not the case though it doesn’t stop people from using 0.5.</p>
<p>We might first wish to look at the distribution of the returned probabilities before making a decision about where to set the threshold. We can see clearly that selecting 0.5 in this case would not be appropriate.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">boxplot</span>(probs, 
        <span class="dt">main=</span><span class="st">&quot;Probabilities from our GLM Model&quot;</span>)
<span class="kw">grid</span>()</code></pre></div>
<p><img src="SEMINAR_SERIES_files/figure-html/bxplotalpha-1.png" width="672" /></p>
<p>The median is somewhere around .25 so we could use that for now although we are just guessing.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mypreds &lt;-<span class="st"> </span><span class="kw">ifelse</span>(probs <span class="op">&gt;</span><span class="st"> </span><span class="fl">0.25</span>,<span class="st">&quot;pos&quot;</span>,<span class="st">&quot;neg&quot;</span>)
mypreds &lt;-<span class="st"> </span><span class="kw">factor</span>(mypreds, <span class="dt">levels =</span> <span class="kw">levels</span>(test[[<span class="st">&quot;diabetes&quot;</span>]]))
mypreds[<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>]</code></pre></div>
<pre><code>##   2   3   9  12  13  17  18  23  25  31 
## neg pos pos pos pos pos neg pos pos pos 
## Levels: neg pos</code></pre>
</div>
<div id="confusion-matrices" class="section level3">
<h3><span class="header-section-number">2.6.2</span> Confusion Matrices</h3>
<p>Next, we would compare our predictions against the known outcomes which are stored in the test data frame:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># How does this compare to the truth ?</span>
<span class="kw">table</span>(<span class="dt">predicted =</span> mypreds,
      <span class="dt">actual =</span> test<span class="op">$</span>diabetes)</code></pre></div>
<pre><code>##          actual
## predicted neg pos
##       neg  60   7
##       pos  37  50</code></pre>
<p>What we are doing is building a “Confusion Matrix” which can help us determine how effective our model is. From such a matrix table we can compute a number of “performance measures”, such as accuracy, precision, sensitivity, specificity and others, to help assess the quality of the model. In predictive modeling we are always interested in how well any given model will perform on “new” data.</p>
<p>There are some functions that can help us compute a confusion matrix. Because the variable we are trying to predict, (diabetes), is a two level factor, (“neg” or “pos”) we’ll need to turn our predictions into a comparable factor. Right now, it’s just a character string.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># test$diabetes &lt;- ordered(test$diabetes,c(&quot;pos&quot;,&quot;neg&quot;))</span>

mypreds &lt;-<span class="st"> </span><span class="kw">factor</span>(mypreds,
                  <span class="dt">levels=</span><span class="kw">levels</span>(test<span class="op">$</span>diabetes))

caret<span class="op">::</span><span class="kw">confusionMatrix</span>(mypreds,test<span class="op">$</span>diabetes,<span class="dt">positive=</span><span class="st">&quot;pos&quot;</span>)</code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction neg pos
##        neg  60   7
##        pos  37  50
##                                          
##                Accuracy : 0.7143         
##                  95% CI : (0.636, 0.7841)
##     No Information Rate : 0.6299         
##     P-Value [Acc &gt; NIR] : 0.01718        
##                                          
##                   Kappa : 0.4472         
##                                          
##  Mcnemar&#39;s Test P-Value : 1.232e-05      
##                                          
##             Sensitivity : 0.8772         
##             Specificity : 0.6186         
##          Pos Pred Value : 0.5747         
##          Neg Pred Value : 0.8955         
##              Prevalence : 0.3701         
##          Detection Rate : 0.3247         
##    Detection Prevalence : 0.5649         
##       Balanced Accuracy : 0.7479         
##                                          
##        &#39;Positive&#39; Class : pos            
## </code></pre>
</div>
<div id="performance-measures" class="section level3">
<h3><span class="header-section-number">2.6.3</span> Performance Measures</h3>
<p>This is helpful stuff although there are a number of measures to select as a primary performance metric. Ideally, we would already know which performance metric we would select to effectively “judge” the quality of our model. In medical tests, “sensitivity” and “specificity” are commonly used. Some applications use “Accuracy” (which isn’t good when there is large group imbalance). Anyway, if, for example, we pick “sensitivity” as a judge of model quality we see that is somewhere around .87. (A much deeper discussion about selecting the best performance measure is in order but we’ll keep moving for mow)</p>
<p>The problem here is that all we have done is looked at the confusion matrix corresponding to one specific (and arbitrary) threshold value when what we need is to look at a number of confusion matrices corresponding to many different thresholds. For example, we might get a better sensitivity level had we selected the mean of the returned probabilities. This process could go on and on and on… So we would benefit from a rigorous approach to find the “best” threshold.</p>
</div>
<div id="the-roc-curve" class="section level3">
<h3><span class="header-section-number">2.6.4</span> The ROC curve</h3>
<p>One way to do this is to use something known as the ROC curve. Luckily, R has functions to do this. This isn’t surprising as it is a standard tool that has been in use for decades long before the hype of AI and ML was around. The ROC curve gives us a “one stop shop” for estimating a value of alpha that results in maximal area under a curve.</p>
<p>In fact, maximizing the area under a given ROC curve winds up being an effective way to judge the differences between one method and another. So, if we wanted to compare the glm model against a Support Vector Machine model, we could use the respective AUC (Area Under Curve) metric to help us. This isn’t the only way to do this but it’s reasonable for now.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">pred &lt;-<span class="st"> </span>ROCR<span class="op">::</span><span class="kw">prediction</span>(<span class="dt">predictions =</span> probs,
                         <span class="dt">labels =</span> test<span class="op">$</span>diabetes)

perf &lt;-<span class="st"> </span><span class="kw">performance</span>(pred,
                    <span class="st">&quot;tpr&quot;</span>,
                    <span class="st">&quot;fpr&quot;</span>)
<span class="kw">plot</span>(perf,<span class="dt">colorize=</span>T,
     <span class="dt">print.cutoffs.at=</span><span class="kw">seq</span>(<span class="dv">0</span>,<span class="dv">1</span>,<span class="dt">by=</span><span class="fl">0.1</span>),
     <span class="dt">lwd=</span><span class="dv">3</span>,<span class="dt">las=</span><span class="dv">1</span>,<span class="dt">main=</span><span class="st">&quot;Cool ROC Curve&quot;</span>)
<span class="kw">abline</span>(<span class="dt">a =</span> <span class="dv">0</span>, <span class="dt">b =</span> <span class="dv">1</span>)

<span class="kw">grid</span>()</code></pre></div>
<p><img src="SEMINAR_SERIES_files/figure-html/rocrcalc-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">myroc &lt;-<span class="st"> </span><span class="kw">performance</span>(pred,<span class="dt">measure=</span><span class="st">&quot;auc&quot;</span>)
myroc<span class="op">@</span>y.values[[<span class="dv">1</span>]]</code></pre></div>
<pre><code>## [1] 0.8507868</code></pre>
<p>So what value of alpha corresponds to the stated max AUC of .80 ? We’ll have to dig into the performance object to get that but it looks to be between 0.30 and 0.40. Note that this is somewhat academic since knowing the max AUC alone helps us decide if our model is any “good”. For completeness we could use another R function to nail this down:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(pROC)
proc &lt;-<span class="st"> </span><span class="kw">roc</span>(test<span class="op">$</span>diabetes,probs)
<span class="kw">round</span>(<span class="kw">coords</span>(proc, <span class="st">&quot;b&quot;</span>, <span class="dt">ret=</span><span class="st">&quot;t&quot;</span>, <span class="dt">transpose =</span> <span class="ot">FALSE</span>),<span class="dv">2</span>)</code></pre></div>
<pre><code>## [1] 0.35</code></pre>
</div>
<div id="other-methods" class="section level3">
<h3><span class="header-section-number">2.6.5</span> Other Methods ?</h3>
<p>Well, we could use another method to see if it yields better performance as determined by the AUC ? Let’s use the <strong>ranger</strong> function which is a fast implementation of random forests. One thing you will notice is that we need to include the “probability” argument in the call to ranger to get the necessary probabilities for computing the AUC. This is one of the aggravations with using different functions. They all have their own peculiar way of doing things.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(ranger)
ranger_mod &lt;-<span class="st"> </span><span class="kw">ranger</span>(diabetes <span class="op">~</span><span class="st"> </span>.,
                   <span class="dt">data =</span> train,
                   <span class="dt">probability =</span> <span class="ot">TRUE</span>,<span class="dt">mtry=</span><span class="dv">4</span>)

<span class="co"># Returns probabilities</span>
ranger_pred &lt;-<span class="st"> </span><span class="kw">predict</span>(ranger_mod,<span class="dt">data=</span>test)

myroc &lt;-<span class="st"> </span><span class="kw">roc</span>(test<span class="op">$</span>diabetes,
             ranger_pred<span class="op">$</span>predictions[,<span class="dv">2</span>])

myroc<span class="op">$</span>auc</code></pre></div>
<pre><code>## Area under the curve: 0.8502</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">pred &lt;-<span class="st"> </span>ROCR<span class="op">::</span><span class="kw">prediction</span>(ranger_pred<span class="op">$</span>predictions[,<span class="dv">2</span>],
                         test<span class="op">$</span>diabetes)
perf &lt;-<span class="st"> </span><span class="kw">performance</span>(pred,
                    <span class="st">&quot;tpr&quot;</span>,
                    <span class="st">&quot;fpr&quot;</span>)
<span class="kw">plot</span>(perf,<span class="dt">colorize=</span>T,
        <span class="dt">print.cutoffs.at=</span><span class="kw">seq</span>(<span class="dv">0</span>,<span class="dv">1</span>,<span class="dt">by=</span><span class="fl">0.1</span>),
     <span class="dt">lwd=</span><span class="dv">3</span>,<span class="dt">las=</span><span class="dv">1</span>,<span class="dt">main=</span><span class="st">&quot;Cool ROC Curve&quot;</span>)
<span class="kw">abline</span>(<span class="dt">a =</span> <span class="dv">0</span>, <span class="dt">b =</span> <span class="dv">1</span>)

<span class="kw">grid</span>()</code></pre></div>
<p><img src="SEMINAR_SERIES_files/figure-html/range1-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">rroc &lt;-<span class="st"> </span><span class="kw">performance</span>(pred,<span class="dt">measure=</span><span class="st">&quot;auc&quot;</span>)
rroc<span class="op">@</span>y.values[[<span class="dv">1</span>]]</code></pre></div>
<pre><code>## [1] 0.8502442</code></pre>
<p>It turns out that this didn’t appear to improve things - at least with one invocation of the method.</p>
</div>
</div>
<div id="improving-the-models" class="section level2">
<h2><span class="header-section-number">2.7</span> Improving The Model(s)</h2>
<p>We haven’t accomplished very much here because we need to look at multiple versions of the data in case we sampled a number of outliers in the creation of our training data. Or, maybe we have excluded a large number of outliers in the training set so they wound up in the test data set which means that the predictive power of our model isn’t as robust as it should be.</p>
<p>Our next steps should involve creating multiple versions of the training and test pairs (say 3 times), compute the optimal AUC, and then look at how those values vary for each of those individual versions. If the AUCs vary widely then maybe our model is over training. If it’s not varying widely, it could be that that the model has high bias.</p>
<div id="cross-fold-validation" class="section level3">
<h3><span class="header-section-number">2.7.1</span> Cross Fold Validation</h3>
<p>This is a method that gives us multiple estimates of out-of-sample error, rather than a single estimate. In particular, we’ll use an approach called “K-Fold Cross Validation” where we will partition our data into 3 individual “folds”&quot; which are basically equal in size. Then we’ll create a loop that does the following:</p>
<ul>
<li>Combines 2 of the folds into a training data set</li>
<li>Builds a model on the combined 2-folds data</li>
<li>Applies the model to holdout fold</li>
<li>Computes the AUC value and stores it</li>
</ul>
<p>Each fold is simply a portion of the data. We’ll generate a list called “folds” that contains 3 elements each of which are 256 index elements corresponding to rows in pm. The way we did the sample insures that each row shows up only in one fold.</p>
<div class="figure">
<img src="PICS/crossk.png" />

</div>
<p>To drive this home, and in case the graphic didn’t help, consider the following simple data frame:</p>
<pre><code>##   id   m1   m2   m3
## 1 a1 0.89 0.36 0.10
## 2 b2 0.75 0.27 0.22
## 3 c3 0.98 0.85 0.95
## 4 d4 0.04 0.36 0.75
## 5 e5 0.90 0.30 0.82
## 6 f6 0.87 0.76 0.42
## 7 g7 0.78 0.84 0.59
## 8 h8 0.38 0.46 0.80
## 9 i9 0.04 0.73 0.89</code></pre>
<p>If we created three folds out of this data frame it would look like the following:</p>
<div class="figure">
<img src="PICS/crossfold.png" />

</div>
<p>Here is our function to implement the K-Fold validation. It’s pretty straightforward to define in terms of coding though it winds up being somewhat specific to the particular method we are using.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">cross_fold &lt;-<span class="st"> </span><span class="cf">function</span>(<span class="dt">numofolds =</span> <span class="dv">3</span>) {
  
  <span class="co"># Function to Do Cross fold validation</span>
  
  <span class="co"># Split the data into K folds (numofolds)</span>
  
  folds &lt;-<span class="st"> </span><span class="kw">split</span>(<span class="kw">sample</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(pm)),<span class="dv">1</span><span class="op">:</span>numofolds) 
  
  <span class="co"># We setup some blank lists to stash results</span>
  folddf    &lt;-<span class="st"> </span><span class="kw">list</span>()  <span class="co"># Contains folds</span>
  modl      &lt;-<span class="st"> </span><span class="kw">list</span>()  <span class="co"># Hold each of the K models</span>
  predl     &lt;-<span class="st"> </span><span class="kw">list</span>()  <span class="co"># Hold rach of the K predictions</span>
  auc       &lt;-<span class="st"> </span><span class="kw">list</span>()  <span class="co"># Hold the auc for a given model</span>
  
  <span class="co"># Create a formula that can be used across multiple</span>
  <span class="co"># iterations through the loop. </span>
  
  myform &lt;-<span class="st"> &quot;diabetes ~ .&quot;</span>
  
  <span class="cf">for</span> (ii <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(folds)) {
    
    <span class="co"># This list holds the actual model we create for each of the </span>
    <span class="co"># 10 folds</span>
    
    modl[[ii]] &lt;-<span class="st"> </span><span class="kw">glm</span>(<span class="dt">formula =</span> myform, 
                      <span class="dt">data =</span> pm[<span class="op">-</span>folds[[ii]],],
                      <span class="dt">family =</span> <span class="st">&quot;binomial&quot;</span>
    )
    
    <span class="co"># This list will contain / hold the models build on the fold</span>
    
    predl[[ii]]  &lt;-<span class="st"> </span><span class="kw">predict</span>(modl[[ii]],
                            <span class="dt">newdata=</span>pm[folds[[ii]],],
                            <span class="dt">type=</span><span class="st">&quot;response&quot;</span>)
    
    <span class="co"># This list will hold the results of the AUC per iteration</span>
    
    pred &lt;-<span class="st"> </span>ROCR<span class="op">::</span><span class="kw">prediction</span>(predl[[ii]],
                             pm[folds[[ii]],]<span class="op">$</span>diabetes)
    
    roc  &lt;-<span class="st"> </span><span class="kw">performance</span>(pred,<span class="dt">measure=</span><span class="st">&quot;auc&quot;</span>)
    auc[[ii]] &lt;-<span class="st"> </span>roc<span class="op">@</span>y.values[[<span class="dv">1</span>]]
  }
  <span class="kw">return</span>(<span class="kw">unlist</span>(auc))
}</code></pre></div>
<p>Running this is now quite simple. By default, this function will loop three times corresponding to the number of folds. During each iteration it will:</p>
<ul>
<li>use glm to build a model on the training folds</li>
<li>create a prediction object using the training fold</li>
<li>compute the underlying AUC associated with the prediction</li>
<li>store the AUC in a vector</li>
</ul>
<p>At the end of the function, the vector containing the computed AUCs will be returned.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">cross_fold</span>()</code></pre></div>
<pre><code>## [1] 0.8628023 0.8050426 0.8115884</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Use more folds</span>

<span class="kw">cross_fold</span>(<span class="dv">8</span>)</code></pre></div>
<pre><code>## [1] 0.7619048 0.8842505 0.8476583 0.8727273 0.8866224 0.7475586 0.8633959
## [8] 0.7528463</code></pre>
<p>We could take the average of the AUCs to get a sense of how well this method would apply to unseen data.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">stripplot</span>(<span class="kw">cross_fold</span>(<span class="dv">8</span>),
          <span class="dt">main=</span><span class="st">&quot;AUC values for K-Fold Validation&quot;</span>,
          <span class="dt">type=</span><span class="kw">c</span>(<span class="st">&quot;g&quot;</span>,<span class="st">&quot;p&quot;</span>),<span class="dt">pch=</span><span class="dv">19</span>,<span class="dt">cex=</span><span class="fl">1.5</span>)</code></pre></div>
<p><img src="SEMINAR_SERIES_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
</div>
<div id="is-there-a-better-way" class="section level3">
<h3><span class="header-section-number">2.7.2</span> Is There a Better Way ?</h3>
<p>In R, as well as with Python, there are a growing number of packages available to help simplify repetitive processes. Building predictive models is no exception especially given that so many sub processes are involved such as splitting data, building a model, making a prediction, comparing it to the labelled data, and so on. The <strong>caret</strong> package provides an easy point of entry into the world of predictive modeling. It provides the following features:</p>
<pre><code>- Streamlined and consistent syntax for more than 
  200 different models
- Can implement any of the 238 different methods using a single function
- Easy data splitting to simplify the creation of train / test pairs
- Realistic model estimates through built-in resampling
- Convenient feature importance determination
- Easy selection of different performance metrics (e.g. &quot;ROC&quot;,&quot;Accuracy&quot;, &quot;Sensitivity&quot;)
- Automated and semi-automated parameter tuning
- Simplifed comparison of different models</code></pre>
<p>The caret package was designed specifically for predictive modeling and, in particular, to provide an intuitive approach to creating, managing, and comparing different models emerging from various methods. Let’s work through our previous examples using functions from <strong>caret</strong>.</p>
</div>
<div id="data-splitting-using-caret" class="section level3">
<h3><span class="header-section-number">2.7.3</span> Data Splitting Using Caret</h3>
<p>Let’s split the data into a training / test pair. The caret package provides some useful functions for this one of which is the <strong>CreateDataPartion</strong> function.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">idx &lt;-<span class="st"> </span><span class="kw">createDataPartition</span>(pm<span class="op">$</span>diabetes,
                           <span class="dt">p=</span>.<span class="dv">80</span>,
                           <span class="dt">list=</span><span class="ot">FALSE</span>)

Train &lt;-<span class="st"> </span>pm[idx,]
Test  &lt;-<span class="st"> </span>pm[<span class="op">-</span>idx,]

<span class="kw">nrow</span>(Train)</code></pre></div>
<pre><code>## [1] 615</code></pre>
<p>Now we can use this to create a training object with caret. We’ll create a GLM model similar to the one we’ve already created. The primary, and most frequently used, function in <strong>caret</strong> is the <strong>train</strong> function. In this example we’ll use it build a model using the <strong>glm</strong> method. We will also specify that “Accuracy” will be the preferred performance measure.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">myglm_caret &lt;-<span class="st"> </span><span class="kw">train</span>(diabetes <span class="op">~</span><span class="st"> </span>.,
                     <span class="dt">data =</span> Train,
                     <span class="dt">method =</span> <span class="st">&quot;glm&quot;</span>,
                     <span class="dt">metric =</span> <span class="st">&quot;Accuracy&quot;</span>)
myglm_caret</code></pre></div>
<pre><code>## Generalized Linear Model 
## 
## 615 samples
##   8 predictor
##   2 classes: &#39;neg&#39;, &#39;pos&#39; 
## 
## No pre-processing
## Resampling: Bootstrapped (25 reps) 
## Summary of sample sizes: 615, 615, 615, 615, 615, 615, ... 
## Resampling results:
## 
##   Accuracy   Kappa    
##   0.7690868  0.4716828</code></pre>
</div>
<div id="specifying-control-options" class="section level3">
<h3><span class="header-section-number">2.7.4</span> Specifying Control Options</h3>
<p>We can even request cross fold validation without having to write our own function to do this. To do this requires the specification of a “control” object which contains information that we would like for the <strong>train</strong> function to consider as it does its work.</p>
<p>Not every invocation of <strong>train</strong> requires an associated <strong>trainControl</strong> object although as you become more experienced building models, you will frequently use this approach.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">control &lt;-<span class="st"> </span><span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;cv&quot;</span>, <span class="dt">number =</span> <span class="dv">5</span>)

myglm_caret &lt;-<span class="st"> </span><span class="kw">train</span>(diabetes <span class="op">~</span><span class="st"> </span>.,
                     <span class="dt">data =</span> Train,
                     <span class="dt">method =</span> <span class="st">&quot;glm&quot;</span>,
                     <span class="dt">metric =</span> <span class="st">&quot;Accuracy&quot;</span>,
                     <span class="dt">trControl =</span> control)
myglm_caret</code></pre></div>
<pre><code>## Generalized Linear Model 
## 
## 615 samples
##   8 predictor
##   2 classes: &#39;neg&#39;, &#39;pos&#39; 
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 492, 492, 492, 492, 492 
## Resampling results:
## 
##   Accuracy   Kappa    
##   0.7674797  0.4643553</code></pre>
</div>
<div id="inspecting-the-model" class="section level3">
<h3><span class="header-section-number">2.7.5</span> Inspecting The Model</h3>
<p>The object returned by caret has a great deal of information packed into it much of which is there to support reproducibility. Some key aspects of the object include, in this case, the Accuracy computation for each fold.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">myglm_caret<span class="op">$</span>resample</code></pre></div>
<pre><code>##    Accuracy     Kappa Resample
## 1 0.7886179 0.5249554    Fold1
## 2 0.7317073 0.3582609    Fold2
## 3 0.7479675 0.4366967    Fold3
## 4 0.7967480 0.5249498    Fold4
## 5 0.7723577 0.4769137    Fold5</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">myglm_caret<span class="op">$</span>results</code></pre></div>
<pre><code>##   parameter  Accuracy     Kappa AccuracySD    KappaSD
## 1      none 0.7674797 0.4643553 0.02732965 0.06986202</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Note that the final reported Accuracy metric is simply the average of </span>
<span class="co"># the reported Accuracy values for each fold</span>

myglm_caret<span class="op">$</span>results[<span class="dv">2</span>] <span class="op">==</span><span class="st"> </span><span class="kw">mean</span>(myglm_caret<span class="op">$</span>resample<span class="op">$</span>Accuracy)</code></pre></div>
<pre><code>##   Accuracy
## 1     TRUE</code></pre>
<p>Of course, you can always look at the model itself to get summary information just as you could if you were not using the <strong>train</strong> function. That is, the <strong>caret</strong> package does not try to conceal or replace what could be done using standard approaches. It overlays the model with information in a way that is transparent.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(myglm_caret)</code></pre></div>
<pre><code>## 
## Call:
## NULL
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.6911  -0.7180  -0.3816   0.6973   2.8206  
## 
## Coefficients:
##               Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) -8.8332681  0.8384820 -10.535  &lt; 2e-16 ***
## pregnant     0.1341329  0.0361450   3.711 0.000206 ***
## glucose      0.0375576  0.0042350   8.868  &lt; 2e-16 ***
## pressure    -0.0169135  0.0059964  -2.821 0.004793 ** 
## triceps      0.0001766  0.0078457   0.023 0.982042    
## insulin     -0.0011989  0.0009865  -1.215 0.224263    
## mass         0.0956998  0.0175534   5.452 4.98e-08 ***
## pedigree     1.0844179  0.3381365   3.207 0.001341 ** 
## age          0.0168712  0.0105600   1.598 0.110120    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 796.05  on 614  degrees of freedom
## Residual deviance: 563.38  on 606  degrees of freedom
## AIC: 581.38
## 
## Number of Fisher Scoring iterations: 5</code></pre>
</div>
<div id="how-well-did-it-perform" class="section level3">
<h3><span class="header-section-number">2.7.6</span> How Well Did It Perform ?</h3>
<p>Remember that one of the features of using caret is that it can help us estimate the out of sample error we will experience when applying our model to new or unseen data. The above model provides an estimate of out of band accuracy as .77</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">myglm_caret</code></pre></div>
<pre><code>## Generalized Linear Model 
## 
## 615 samples
##   8 predictor
##   2 classes: &#39;neg&#39;, &#39;pos&#39; 
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 492, 492, 492, 492, 492 
## Resampling results:
## 
##   Accuracy   Kappa    
##   0.7674797  0.4643553</code></pre>
<p>Let’s create a prediction object using the Test data to see how close we came to this estimate. It’s bad and it’s a little worse than caret’s estimate which is to be expected. Also, this is just an estimate using a single threshold when using the predict function. There are more sophisticated ways to estimate the accuracy.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mypreds_glm &lt;-<span class="st"> </span><span class="kw">predict</span>(myglm_caret, Test)

<span class="co"># Create a Table of known outcomes vs the predicted outcomes</span>
outcome &lt;-<span class="st"> </span><span class="kw">table</span>(<span class="dt">preds=</span>mypreds_glm,<span class="dt">actual=</span>Test<span class="op">$</span>diabetes)</code></pre></div>
</div>
<div id="comparing-performance-across-other-methods" class="section level3">
<h3><span class="header-section-number">2.7.7</span> Comparing Performance Across Other Methods</h3>
<p>The advantage of the <strong>train</strong> function is that we can use the same control objects across a number of modeling techniques which then makes it easier to compare performance across various methods.</p>
<p>As an example, instead of using the “glm” method we could pick another one such as Decision Tree. All we need to know is the name of the method we want. A complete list of supported models listed by category can be found <a href="https://topepo.github.io/caret/available-models.html">here</a></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">control &lt;-<span class="st"> </span><span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;cv&quot;</span>, <span class="dt">number =</span> <span class="dv">5</span>)

myrpart_caret &lt;-<span class="st"> </span><span class="kw">train</span>(diabetes <span class="op">~</span><span class="st"> </span>.,
                     <span class="dt">data =</span> Train,
                     <span class="dt">method =</span> <span class="st">&quot;rpart&quot;</span>,
                     <span class="dt">metric =</span> <span class="st">&quot;Accuracy&quot;</span>,
                     <span class="dt">trControl =</span> control)
myrpart_caret</code></pre></div>
<pre><code>## CART 
## 
## 615 samples
##   8 predictor
##   2 classes: &#39;neg&#39;, &#39;pos&#39; 
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 492, 492, 492, 492, 492 
## Resampling results across tuning parameters:
## 
##   cp          Accuracy   Kappa    
##   0.01162791  0.7398374  0.4308597
##   0.03100775  0.7528455  0.4159712
##   0.29302326  0.6975610  0.1993799
## 
## Accuracy was used to select the optimal model using the largest value.
## The final value used for the model was cp = 0.03100775.</code></pre>
<p>This method employs a Decision Tree approach which also involves use of “hyper parameters”. However, at this point we don’t really need to know much about those (although we should) when selecting the method. The larger point is that all we need to know is the name of the alternative method and we can reuse the previous <strong>control</strong> object.</p>
</div>
<div id="different-performance-measures" class="section level3">
<h3><span class="header-section-number">2.7.8</span> Different Performance Measures</h3>
<p>Not only can we easily select different methods we can also select different performance measures. It does require changes to the control object and arguments to the <strong>train</strong> function though we do not need to read the underlying help pages for a given method to do this. This is a true convenience and time saver that makes reproducing these experiments much easier.</p>
<p>In this example we want to use the “Area Under Curve” (AUC) metric that comes from an associated ROC curve. To do this will require the model to generate class probabilities from which to build the ROC curve so this information needs to be specified in the control object.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">control &lt;-<span class="st"> </span><span class="kw">trainControl</span>(<span class="dt">classProbs =</span> <span class="ot">TRUE</span>,
                        <span class="dt">summaryFunction =</span> twoClassSummary,
                        <span class="dt">method =</span> <span class="st">&quot;cv&quot;</span>,
                        <span class="dt">number =</span> <span class="dv">8</span>)

myglm_caret_roc &lt;-<span class="st"> </span><span class="kw">train</span>(diabetes <span class="op">~</span><span class="st"> </span>.,
                         <span class="dt">data =</span> Train,
                         <span class="dt">method =</span> <span class="st">&quot;glm&quot;</span>,
                         <span class="dt">metric =</span> <span class="st">&quot;ROC&quot;</span>,
                         <span class="dt">trControl =</span> control)

myglm_caret_roc</code></pre></div>
<pre><code>## Generalized Linear Model 
## 
## 615 samples
##   8 predictor
##   2 classes: &#39;neg&#39;, &#39;pos&#39; 
## 
## No pre-processing
## Resampling: Cross-Validated (8 fold) 
## Summary of sample sizes: 538, 538, 539, 538, 538, 538, ... 
## Resampling results:
## 
##   ROC        Sens  Spec     
##   0.8435613  0.87  0.6000712</code></pre>
<p>This is a true convenience and we don’t have to use a separate R package to compute the AUC. It becomes a by product of the modeling process.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">control &lt;-<span class="st"> </span><span class="kw">trainControl</span>(<span class="dt">classProbs =</span> <span class="ot">TRUE</span>,
                        <span class="dt">summaryFunction =</span> twoClassSummary)

myglm &lt;-<span class="st"> </span><span class="kw">train</span>(diabetes <span class="op">~</span><span class="st"> </span>.,
               <span class="dt">data =</span> Train,
               <span class="dt">method =</span> <span class="st">&quot;glm&quot;</span>,
               <span class="dt">metric =</span> <span class="st">&quot;ROC&quot;</span>,
               <span class="dt">trControl =</span> control)

myglm</code></pre></div>
<pre><code>## Generalized Linear Model 
## 
## 615 samples
##   8 predictor
##   2 classes: &#39;neg&#39;, &#39;pos&#39; 
## 
## No pre-processing
## Resampling: Bootstrapped (25 reps) 
## Summary of sample sizes: 615, 615, 615, 615, 615, 615, ... 
## Resampling results:
## 
##   ROC        Sens       Spec    
##   0.8406046  0.8802542  0.583543</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">control &lt;-<span class="st"> </span><span class="kw">trainControl</span>(<span class="dt">classProbs =</span> <span class="ot">TRUE</span>,
                        <span class="dt">summaryFunction =</span> twoClassSummary,
                        <span class="dt">method =</span> <span class="st">&quot;cv&quot;</span>,
                        <span class="dt">number =</span> <span class="dv">8</span>)

myglm_caret_roc &lt;-<span class="st"> </span><span class="kw">train</span>(diabetes <span class="op">~</span><span class="st"> </span>.,
                         <span class="dt">data =</span> Train,
                         <span class="dt">method =</span> <span class="st">&quot;glm&quot;</span>,
                         <span class="dt">metric =</span> <span class="st">&quot;ROC&quot;</span>,
                         <span class="dt">trControl =</span> control)

myglm_caret_roc</code></pre></div>
<pre><code>## Generalized Linear Model 
## 
## 615 samples
##   8 predictor
##   2 classes: &#39;neg&#39;, &#39;pos&#39; 
## 
## No pre-processing
## Resampling: Cross-Validated (8 fold) 
## Summary of sample sizes: 538, 539, 538, 538, 538, 538, ... 
## Resampling results:
## 
##   ROC       Sens  Spec     
##   0.846339  0.88  0.5997151</code></pre>
<p>Specifically, the control object can remain the same across different methods assuming that we wish to continue with classification. Here, we’ll use Random Forests which are a generalization beyond a single Decision Tree.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># The following is the same control object from before</span>
control &lt;-<span class="st"> </span><span class="kw">trainControl</span>(<span class="dt">classProbs =</span> <span class="ot">TRUE</span>,
                        <span class="dt">summaryFunction =</span> twoClassSummary,
                        <span class="dt">method =</span> <span class="st">&quot;cv&quot;</span>,
                        <span class="dt">number =</span> <span class="dv">8</span>)
<span class="co"># We&#39;ll </span>
myrf_caret &lt;-<span class="st"> </span><span class="kw">train</span>(diabetes <span class="op">~</span><span class="st"> </span>.,
               <span class="dt">data =</span> Train,
 <span class="co">#              method = &quot;svmLinear&quot;,</span>
               <span class="dt">method =</span> <span class="st">&quot;rf&quot;</span>,
               <span class="dt">metric =</span> <span class="st">&quot;ROC&quot;</span>,
               <span class="dt">trControl =</span> control)

myrf_caret</code></pre></div>
<pre><code>## Random Forest 
## 
## 615 samples
##   8 predictor
##   2 classes: &#39;neg&#39;, &#39;pos&#39; 
## 
## No pre-processing
## Resampling: Cross-Validated (8 fold) 
## Summary of sample sizes: 538, 538, 538, 538, 538, 538, ... 
## Resampling results across tuning parameters:
## 
##   mtry  ROC        Sens    Spec     
##   2     0.8334348  0.8475  0.6047009
##   5     0.8289156  0.8325  0.6095085
##   8     0.8211895  0.8150  0.6324786
## 
## ROC was used to select the optimal model using the largest value.
## The final value used for the model was mtry = 2.</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(myrf_caret)</code></pre></div>
<p><img src="SEMINAR_SERIES_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
</div>
<div id="feature-importance" class="section level3">
<h3><span class="header-section-number">2.7.9</span> Feature Importance</h3>
<p>Another advantage of using the caret <strong>train</strong> function is that it provides a method to determine variable importance. This is useful when considering what features to include or not when building a model. If we summarize a given model, our myglm_caret model, we’ll see that some of our predictors are not significant.</p>
<p>We could use the <strong>varImp</strong> to use statistics generated by the specific modeling process itself. For more complex modeling techniques this winds up being very useful since digging into the model diagnostics can be daunting - although quite useful.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">varImp</span>(myglm_caret)</code></pre></div>
<pre><code>## glm variable importance
## 
##          Overall
## glucose   100.00
## mass       61.38
## pregnant   41.70
## pedigree   36.00
## pressure   31.63
## age        17.81
## insulin    13.48
## triceps     0.00</code></pre>
<p>If you wanted to see how the different models rates the significance of predictor variables then you can easily plot them.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(gridExtra)</code></pre></div>
<pre><code>## 
## Attaching package: &#39;gridExtra&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:dplyr&#39;:
## 
##     combine</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">p1 &lt;-<span class="st"> </span><span class="kw">plot</span>(<span class="kw">varImp</span>(myglm_caret),<span class="dt">main=</span><span class="st">&quot;varImp for glm&quot;</span>)
p2 &lt;-<span class="st"> </span><span class="kw">plot</span>(<span class="kw">varImp</span>(myrf_caret),<span class="dt">main=</span><span class="st">&quot;varImp for Rf&quot;</span>)
<span class="kw">grid.arrange</span>(p1,p2,<span class="dt">ncol=</span><span class="dv">2</span>)</code></pre></div>
<p><img src="SEMINAR_SERIES_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
</div>
<div id="feature-elimination" class="section level3">
<h3><span class="header-section-number">2.7.10</span> Feature Elimination</h3>
<p>The caret package also supports “recursive feature elimination” which automates the selection of optimal features. This can be controversial since such a process could work at the expense of important statistical considerations. However, it remains a tool in the Machine Learning toolbox. Let’s work though an example of this using caret functions. First, we’ll remove highly correlated predictor variables from consideration.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">correlationMatrix &lt;-<span class="st"> </span><span class="kw">cor</span>(pm[,<span class="dv">1</span><span class="op">:</span><span class="dv">8</span>])

<span class="co"># summarize the correlation matrix</span>
<span class="kw">print</span>(correlationMatrix)</code></pre></div>
<pre><code>##             pregnant    glucose   pressure     triceps     insulin
## pregnant  1.00000000 0.12945867 0.14128198 -0.08167177 -0.07353461
## glucose   0.12945867 1.00000000 0.15258959  0.05732789  0.33135711
## pressure  0.14128198 0.15258959 1.00000000  0.20737054  0.08893338
## triceps  -0.08167177 0.05732789 0.20737054  1.00000000  0.43678257
## insulin  -0.07353461 0.33135711 0.08893338  0.43678257  1.00000000
## mass      0.01768309 0.22107107 0.28180529  0.39257320  0.19785906
## pedigree -0.03352267 0.13733730 0.04126495  0.18392757  0.18507093
## age       0.54434123 0.26351432 0.23952795 -0.11397026 -0.04216295
##                mass    pedigree         age
## pregnant 0.01768309 -0.03352267  0.54434123
## glucose  0.22107107  0.13733730  0.26351432
## pressure 0.28180529  0.04126495  0.23952795
## triceps  0.39257320  0.18392757 -0.11397026
## insulin  0.19785906  0.18507093 -0.04216295
## mass     1.00000000  0.14064695  0.03624187
## pedigree 0.14064695  1.00000000  0.03356131
## age      0.03624187  0.03356131  1.00000000</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># find attributes that are highly corrected </span>
<span class="co"># (ideally &gt;0.75)</span>
highlyCorrelated &lt;-<span class="st"> </span><span class="kw">findCorrelation</span>(correlationMatrix,
                                    <span class="dt">cutoff=</span><span class="fl">0.5</span>)

<span class="co"># print indexes of highly correlated attributes</span>
<span class="kw">print</span>(highlyCorrelated)</code></pre></div>
<pre><code>## [1] 8</code></pre>
</div>
<div id="the-rfe-function" class="section level3">
<h3><span class="header-section-number">2.7.11</span> The rfe Function</h3>
<p>Let’s apply the RFE method on the Pima Indians Diabetes data set. The algorithm is configured to explore all possible subsets of the attributes. All 8 attributes are selected in this example, although in the plot showing the accuracy of the different attribute subset sizes, we can see that just 4 attributes gives almost comparable results</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">rfFuncs<span class="op">$</span>summary &lt;-<span class="st"> </span>twoClassSummary
control &lt;-<span class="st"> </span><span class="kw">rfeControl</span>(<span class="dt">functions=</span>rfFuncs, 
                      <span class="dt">method=</span><span class="st">&quot;cv&quot;</span>, 
                      <span class="dt">number=</span><span class="dv">8</span>)

<span class="co"># run the RFE algorithm</span>
results &lt;-<span class="st"> </span><span class="kw">rfe</span>(pm[,<span class="dv">1</span><span class="op">:</span><span class="dv">8</span>], 
               pm[,<span class="dv">9</span>], 
               <span class="dt">sizes=</span><span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">8</span>), 
               <span class="dt">rfeControl=</span>control,
               <span class="dt">metric=</span><span class="st">&quot;ROC&quot;</span>)

<span class="co"># summarize the results</span>
<span class="kw">print</span>(results)</code></pre></div>
<pre><code>## 
## Recursive feature selection
## 
## Outer resampling method: Cross-Validated (8 fold) 
## 
## Resampling performance over subset size:
## 
##  Variables    ROC   Sens   Spec   ROCSD  SensSD  SpecSD Selected
##          1 0.7329 0.8580 0.4367 0.05447 0.03128 0.07988         
##          2 0.7692 0.8218 0.5556 0.04496 0.05580 0.09313         
##          3 0.8085 0.8360 0.5183 0.06020 0.07023 0.10237         
##          4 0.8161 0.8279 0.5856 0.06796 0.08844 0.10745         
##          5 0.8265 0.8459 0.5969 0.05945 0.08339 0.11810         
##          6 0.8320 0.8539 0.5893 0.05828 0.06952 0.11063         
##          7 0.8335 0.8519 0.5820 0.05318 0.06846 0.09893         
##          8 0.8365 0.8660 0.5930 0.04717 0.06331 0.11530        *
## 
## The top 5 variables (out of 8):
##    glucose, mass, age, pregnant, insulin</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># list the chosen features</span>
<span class="kw">predictors</span>(results)</code></pre></div>
<pre><code>## [1] &quot;glucose&quot;  &quot;mass&quot;     &quot;age&quot;      &quot;pregnant&quot; &quot;insulin&quot;  &quot;pedigree&quot;
## [7] &quot;triceps&quot;  &quot;pressure&quot;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># plot the results</span>
<span class="kw">plot</span>(results, <span class="dt">type=</span><span class="kw">c</span>(<span class="st">&quot;g&quot;</span>, <span class="st">&quot;o&quot;</span>))</code></pre></div>
<p><img src="SEMINAR_SERIES_files/figure-html/unnamed-chunk-23-1.png" width="672" /></p>
</div>
<div id="comparing-models" class="section level3">
<h3><span class="header-section-number">2.7.12</span> Comparing Models</h3>
<p>One of the more frequent activities in Machine Learning relates to setting up “shoot ours” between different models to see which one will perform the best. This is something we could do without <strong>caret</strong> but the package does help accomplish this using a standard interface. We’ll keep using the Pima Indians Data and (re)build a few models. We’ll use a common <strong>control</strong> object as well as a seed to maintain reproducibility.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">control &lt;-<span class="st"> </span><span class="kw">trainControl</span>(<span class="dt">method=</span><span class="st">&quot;repeatedcv&quot;</span>, 
                        <span class="dt">number=</span><span class="dv">5</span>, 
                        <span class="dt">summaryFunction =</span> twoClassSummary,
                        <span class="dt">classProbs =</span> <span class="ot">TRUE</span>,
                        <span class="dt">repeats=</span><span class="dv">3</span>)

<span class="co"># Train the glm model</span>
<span class="kw">set.seed</span>(<span class="dv">7</span>)
model_glm &lt;-<span class="st"> </span><span class="kw">train</span>(diabetes <span class="op">~</span><span class="st"> </span>., 
                   <span class="dt">data=</span>pm, 
                   <span class="dt">method=</span><span class="st">&quot;glm&quot;</span>, 
                   <span class="dt">metric=</span><span class="st">&quot;ROC&quot;</span>,
                   <span class="dt">trControl=</span>control)

<span class="co"># Train the Decision Tree &lt;odel</span>
<span class="kw">set.seed</span>(<span class="dv">7</span>)
model_rpart &lt;-<span class="st"> </span><span class="kw">train</span>(diabetes<span class="op">~</span>., 
                  <span class="dt">data=</span>pm, 
                  <span class="dt">method=</span><span class="st">&quot;rpart&quot;</span>, 
                  <span class="dt">metric=</span><span class="st">&quot;ROC&quot;</span>,
                  <span class="dt">trControl=</span>control)

<span class="co"># Train the Random Forest model</span>
<span class="kw">set.seed</span>(<span class="dv">7</span>)
model_rf &lt;-<span class="st"> </span><span class="kw">train</span>(diabetes<span class="op">~</span>., 
                  <span class="dt">data=</span>pm, 
                  <span class="dt">method=</span><span class="st">&quot;rf&quot;</span>, 
                  <span class="dt">metric=</span><span class="st">&quot;ROC&quot;</span>,
                  <span class="dt">trControl=</span>control)

<span class="co"># Use the resamples function to prep for comparisons</span>
results &lt;-<span class="st"> </span><span class="kw">resamples</span>(<span class="kw">list</span>(<span class="dt">GLM   =</span> model_glm, 
                          <span class="dt">RPART =</span> model_rpart, 
                          <span class="dt">RF    =</span> model_rf))</code></pre></div>
<p>Now we can easily look at how well the different models compare:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># summarize the distributions</span>
<span class="kw">summary</span>(results)</code></pre></div>
<pre><code>## 
## Call:
## summary.resamples(object = results)
## 
## Models: GLM, RPART, RF 
## Number of resamples: 15 
## 
## ROC 
##            Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA&#39;s
## GLM   0.7885185 0.8087963 0.8179245 0.8307750 0.8472956 0.8818868    0
## RPART 0.6600926 0.7129079 0.7579630 0.7518451 0.7938889 0.8461321    0
## RF    0.7878704 0.8133491 0.8208333 0.8287987 0.8421995 0.8819811    0
## 
## Sens 
##       Min. 1st Qu. Median      Mean 3rd Qu. Max. NA&#39;s
## GLM   0.80   0.845   0.88 0.8786667   0.910 0.94    0
## RPART 0.74   0.815   0.85 0.8360000   0.875 0.90    0
## RF    0.79   0.830   0.87 0.8540000   0.880 0.89    0
## 
## Spec 
##            Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA&#39;s
## GLM   0.4716981 0.5462963 0.5740741 0.5721407 0.5943396 0.7037037    0
## RPART 0.4444444 0.4907407 0.5660377 0.5610995 0.6353948 0.6792453    0
## RF    0.4528302 0.5370370 0.6111111 0.5934778 0.6415094 0.7222222    0</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># boxplots of results</span>
<span class="kw">bwplot</span>(results)</code></pre></div>
<p><img src="SEMINAR_SERIES_files/figure-html/unnamed-chunk-25-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># dot plots of results</span>
<span class="kw">dotplot</span>(results)</code></pre></div>
<p><img src="SEMINAR_SERIES_files/figure-html/unnamed-chunk-25-2.png" width="672" /></p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": ["SEMINAR_SERIES.pdf", "SEMINAR_SERIES.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

</body>

</html>
